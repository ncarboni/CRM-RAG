# Local Embedding Configuration
# Use this config for fast batch embedding with sentence-transformers
# No API costs, no rate limits, GPU acceleration when available

# LLM Provider (for text generation/chat)
# Still need an LLM for answering questions, but embeddings are local
LLM_PROVIDER=openai
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=4096

# Embedding Provider (separate from LLM)
# Options: openai, sentence-transformers (or "local"), ollama
EMBEDDING_PROVIDER=local

# Embedding Model for sentence-transformers
# Recommended models:
# - BAAI/bge-m3: Best quality, multilingual (100+ languages), 1024 dims, ~2.3GB
# - BAAI/bge-base-en-v1.5: Best for English only, 768 dims, ~440MB
# - all-MiniLM-L6-v2: Fast, good quality, 384 dims, ~90MB
# - all-mpnet-base-v2: Good balance, 768 dims, ~420MB
EMBEDDING_MODEL=BAAI/bge-m3

# Batch size for embedding (higher = faster, more memory)
# This is the BASE batch size - dynamic batching will automatically reduce it
# for longer documents to prevent out-of-memory errors.
#
# How dynamic batching works:
# - Documents are sorted by length and grouped into batches
# - Batch size is reduced for longer documents (relative to model's max_seq_length)
# - Short docs (<12.5% of max): full batch_size
# - Medium docs (12.5-25% of max): batch_size ÷ 2
# - Medium-long docs (25-50% of max): batch_size ÷ 4
# - Long docs (50-75% of max): batch_size ÷ 8
# - Very long docs (>75% of max): batch_size ÷ 16
#
# For bge-m3 (max_seq=8192 tokens ≈ 32K chars):
# - batch_size=64 with short docs → 64
# - batch_size=64 with very long docs (>24K chars) → 4
#
# See docs/PROCESSING.md for configuration details.
EMBEDDING_BATCH_SIZE=64

# Device for embedding computation
# Options: auto (detect CUDA/MPS/CPU), cuda, mps (Apple Silicon), cpu
EMBEDDING_DEVICE=auto

# Embedding cache (recommended for large datasets)
# Allows stopping and resuming processing
USE_EMBEDDING_CACHE=true

# Generation temperature
TEMPERATURE=0.7

# Flask port
PORT=5001

# NOTE: SPARQL endpoints are defined in config/datasets.yaml
